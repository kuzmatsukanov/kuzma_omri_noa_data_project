{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d828e9a1-5ff6-4386-89fd-516a305124ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac94bbdf-2f9e-47e4-a077-e13801025cad",
   "metadata": {
    "id": "6I76haoGWtHU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential   #, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Rescaling, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from tensorflow.keras.initializers import GlorotUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5127870-a75b-455b-8bd3-175036579b4a",
   "metadata": {},
   "source": [
    "# Personalized Fashion Recommendations\n",
    "## Baseline model for image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c27f55-e6da-4629-90b4-435424b02fc8",
   "metadata": {},
   "source": [
    "The images are RGB color coded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d424b1-5332-4448-bb30-0ffd34b984ec",
   "metadata": {},
   "source": [
    "### Load and preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8909502f-37d8-48d0-9e57-3c6c9b97918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image paths\n",
    "image_paths = [\"samples/sample_tshirt.jpg\", \"samples/sample_tshirt.jpg\"]\n",
    "\n",
    "# List to store the loaded images\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4632788c-38c2-47fd-a8f7-b85bb9c81a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efec3e56-f152-424a-a6d5-75b3ec508570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and convert them to numpy arrays\n",
    "for path in image_paths:\n",
    "    image = load_img(path=path, target_size=TARGET_SIZE)\n",
    "    image = img_to_array(image)\n",
    "    images.append(image)\n",
    "\n",
    "# Convert the list of images to a numpy array\n",
    "images = np.stack(images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6811b4b-6551-4c87-ba95-3b15b88b08b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 100, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27292438-6da3-4407-8878-f3ad0b321429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake labels\n",
    "labels = np.random.choice(range(NUMBER_OF_CLASSES), size=len(images), replace=True)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509814a-ca34-42b7-85ca-cdd144634c46",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "63a8d43b-2446-43e5-a622-ce86081db2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_train, images_test, labels_train, labels_test = train_test_split(images, labels, train_size=2)\n",
    "\n",
    "images_train = images\n",
    "labels_train = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad15805-a9df-412f-8686-ab5b1cfc98ac",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b8188b2-fc5d-4ebc-82d8-47b1aa80529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c754160-1fdc-4ba3-bf3b-fd8b9034020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 18:17:57.865728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "baseline_model = Sequential(layers=[\n",
    "    # Rescale the image in the [0, 255] range to be in the [0, 1] range\n",
    "    Rescaling(scale=1./255, input_shape=images.shape[1:]),\n",
    "    \n",
    "    # The first convolutional layer\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    \n",
    "    # The max pooling layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the output\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(units=32, activation='relu'),\n",
    "    \n",
    "    # The output layer with units=NUMBER_OF_CLASSES. Sum of outputs equals to 1.\n",
    "    Dense(NUMBER_OF_CLASSES, activation='softmax')\n",
    "], name=\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68999dab-bfd8-4fbd-a213-8da5d12f790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input and output of the model are correct\n",
    "assert baseline_model.input_shape[1:] == images.shape[1:]\n",
    "assert baseline_model.output_shape[1:] == (NUMBER_OF_CLASSES,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf178c01-85f1-407f-abaa-4bda19a44bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9525b899-969c-4ebd-b54e-0315789fd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Early Stopping and Checkpoints\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model_baseline_CNN.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a874fa6-58eb-4317-9519-64abb67ab75c",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f13a8f3-bcd6-4952-b96a-43d52d681fa7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1539 - accuracy: 0.0000e+00\n",
      "Epoch 1: val_loss improved from inf to 19.42424, saving model to best_model_baseline_CNN.h5\n",
      "1/1 [==============================] - 1s 853ms/step - loss: 2.1539 - accuracy: 0.0000e+00 - val_loss: 19.4242 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 2: val_loss did not improve from 19.42424\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 34.6176 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8014e-05 - accuracy: 1.0000\n",
      "Epoch 3: val_loss did not improve from 19.42424\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.8014e-05 - accuracy: 1.0000 - val_loss: 46.8243 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.5763e-07 - accuracy: 1.0000\n",
      "Epoch 4: val_loss did not improve from 19.42424\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5763e-07 - accuracy: 1.0000 - val_loss: 57.1727 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5: val_loss did not improve from 19.42424\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 66.2122 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6: val_loss did not improve from 19.42424\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 74.1974 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce1eb04ca0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(x=images_train, y=labels_train, batch_size=1, epochs=100, validation_split=0.1,\n",
    "          callbacks=[early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61090c-0261-4b93-8330-eaa645c15af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138d604-331f-42ff-b0ca-0e05e3784b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c11014-ba75-4692-84d3-529d94fe3586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ae01f8c-2c80-4714-8744-10699711ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git branch\n",
    "!git add .\n",
    "# !git commit -m \"EDA of article.csv update\"\n",
    "# !git push origin kuzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "02664d8c-47d8-48f1-8203-76b5707fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch kuzma\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mmodified:   .gitignore\u001b[m\n",
      "\t\u001b[32mmodified:   EDA_kuzma.ipynb\u001b[m\n",
      "\t\u001b[32mnew file:   baseline_model_CNN_kuzma.ipynb\u001b[m\n",
      "\t\u001b[32mnew file:   best_model_baseline_CNN.h5\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69165b-b419-4894-b13c-8afd4b6ccdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

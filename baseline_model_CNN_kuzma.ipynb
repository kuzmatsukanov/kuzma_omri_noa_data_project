{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d828e9a1-5ff6-4386-89fd-516a305124ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac94bbdf-2f9e-47e4-a077-e13801025cad",
   "metadata": {
    "id": "6I76haoGWtHU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential   #, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Rescaling, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from tensorflow.keras.initializers import GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "30f82df2-ead0-4316-a924-1d8d66b4e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath(article_id):\n",
    "    \"\"\"\n",
    "    Returns the filepath of the image for the given article_id, e.g. 'samples/10/108775015.jpg'\n",
    "    article_id: (str) article_id e.g. '108775015'\n",
    "    \"\"\"\n",
    "    filepath = 'samples/' + article_id[:2] + '/' + '0' + article_id + '.jpg'\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d68034f0-ab16-4661-ab96-053edc96a397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_files(dir):\n",
    "    \"\"\"\n",
    "    Returns list of filepaths in the given directory (dir) excluding hidden files\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if not file.startswith('.'):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "len(list_files('samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "051bffb6-eab8-42bc-905d-fef89388f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/kuzmatsukanov/.kaggle/kaggle.json'\n",
      "404 - Not Found\n"
     ]
    }
   ],
   "source": [
    "# !kaggle competitions download -c h-and-m-personalized-fashion-recommendations -f \"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5127870-a75b-455b-8bd3-175036579b4a",
   "metadata": {},
   "source": [
    "# Personalized Fashion Recommendations\n",
    "## Baseline model for image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c27f55-e6da-4629-90b4-435424b02fc8",
   "metadata": {},
   "source": [
    "The images are RGB color coded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4825e-2345-415e-86bd-b11816b60ec2",
   "metadata": {},
   "source": [
    "### Load the list of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754ab3b-bbad-4884-b142-118938d115da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88cba49-9a1f-40d8-845d-008432255b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60d096-b94e-421e-bbfc-85b73d99125f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6d4d15c7-4f78-4baa-9670-13367bf5b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filename_product_group_name.txt\", \"r\") as file:\n",
    "    lines = file.readlines()[1:]\n",
    "    data = [line.strip().split(\",\") for line in lines]\n",
    "    df = pd.DataFrame(data, columns=[\"article_id\", \"product_group_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9e292998-fe2b-4926-8494-12d38f61f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_filepath'] = df['article_id'].apply(get_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fcc83-6266-46fa-8447-48411ea9ba9f",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "52ac3a08-20b4-45ce-9b81-453c20e87c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The product_group_name to have\n",
    "categories_to_have = \\\n",
    "['Garment Upper body', 'Garment Lower body', 'Garment Full body', 'Accessories', 'Underwear',\\\n",
    " 'Shoes', 'Swimwear', 'Socks & Tights', 'Nightwear']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(categories_to_have)\n",
    "labels = df['product_group_name']\n",
    "labels = pd.Series(data=le.transform(labels), index=labels.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3189c66-a146-4b62-a9f5-5fa9521ca467",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['article_filepath']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d424b1-5332-4448-bb30-0ffd34b984ec",
   "metadata": {},
   "source": [
    "### Load and preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8909502f-37d8-48d0-9e57-3c6c9b97918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image paths\n",
    "#image_paths = [\"samples/sample_tshirt.jpg\", \"samples/sample_tshirt.jpg\"]\n",
    "image_paths = X[:3].values\n",
    "# List to store the loaded images\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4632788c-38c2-47fd-a8f7-b85bb9c81a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "efec3e56-f152-424a-a6d5-75b3ec508570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and convert them to numpy arrays\n",
    "for path in image_paths:\n",
    "    image = load_img(path=path, target_size=TARGET_SIZE)\n",
    "    image = img_to_array(image)\n",
    "    images.append(image)\n",
    "\n",
    "# Convert the list of images to a numpy array\n",
    "images = np.stack(images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c6811b4b-6551-4c87-ba95-3b15b88b08b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 100, 3)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27292438-6da3-4407-8878-f3ad0b321429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Fake labels\n",
    "# labels = np.random.choice(range(NUMBER_OF_CLASSES), size=len(images), replace=True)\n",
    "# labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509814a-ca34-42b7-85ca-cdd144634c46",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "63a8d43b-2446-43e5-a622-ce86081db2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_train, images_test, labels_train, labels_test = train_test_split(images, labels, train_size=2)\n",
    "\n",
    "images_train = images\n",
    "labels_train = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad15805-a9df-412f-8686-ab5b1cfc98ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b8188b2-fc5d-4ebc-82d8-47b1aa80529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_CLASSES = len(np.unique(labels))\n",
    "NUMBER_OF_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9c754160-1fdc-4ba3-bf3b-fd8b9034020c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "baseline_model = Sequential(layers=[\n",
    "    # Rescale the image in the [0, 255] range to be in the [0, 1] range\n",
    "    Rescaling(scale=1./255, input_shape=images.shape[1:]),\n",
    "    \n",
    "    # The first convolutional layer\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    \n",
    "    # The max pooling layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the output\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(units=32, activation='relu'),\n",
    "    \n",
    "    # The output layer with units=NUMBER_OF_CLASSES. Sum of outputs equals to 1.\n",
    "    Dense(NUMBER_OF_CLASSES, activation='softmax')\n",
    "], name=\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "68999dab-bfd8-4fbd-a213-8da5d12f790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input and output of the model are correct\n",
    "assert baseline_model.input_shape[1:] == images.shape[1:]\n",
    "assert baseline_model.output_shape[1:] == (NUMBER_OF_CLASSES,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bf178c01-85f1-407f-abaa-4bda19a44bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9525b899-969c-4ebd-b54e-0315789fd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Early Stopping and Checkpoints\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model_baseline_CNN.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a874fa6-58eb-4317-9519-64abb67ab75c",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4f13a8f3-bcd6-4952-b96a-43d52d681fa7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4853 - accuracy: 0.0000e+00\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to best_model_baseline_CNN.h5\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 1.2426 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6: val_loss did not improve from 0.00000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce0e0a0d00>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(x=images_train, y=labels_train, batch_size=1, epochs=100, validation_split=0.1,\n",
    "          callbacks=[early_stop, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61090c-0261-4b93-8330-eaa645c15af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35d2bd-749a-471f-84d0-cc1c5aa09791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c11014-ba75-4692-84d3-529d94fe3586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7ae01f8c-2c80-4714-8744-10699711ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kuzma 552bd95] baseline_model_CNN_kuzma_colab.ipynb is most correct\n",
      " 2 files changed, 913 insertions(+), 12 deletions(-)\n",
      " create mode 100644 baseline_model_CNN_kuzma_colab.ipynb\n",
      "Enumerating objects: 6, done.\n",
      "Counting objects: 100% (6/6), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 6.82 KiB | 6.82 MiB/s, done.\n",
      "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/kuzmatsukanov/kuzma_omri_noa_data_project.git\n",
      "   2074ebe..552bd95  kuzma -> kuzma\n"
     ]
    }
   ],
   "source": [
    "# !git branch\n",
    "# !git add .\n",
    "# !git commit -m \"baseline_model_CNN_kuzma_colab.ipynb is most correct\"\n",
    "# !git push origin kuzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69165b-b419-4894-b13c-8afd4b6ccdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

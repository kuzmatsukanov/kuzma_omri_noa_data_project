{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d828e9a1-5ff6-4386-89fd-516a305124ac",
   "metadata": {
    "id": "d828e9a1-5ff6-4386-89fd-516a305124ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "E_OwxrrrhZ3w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_OwxrrrhZ3w",
    "outputId": "28026176-ffc7-4ea3-9738-98e790a09724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "G4CD3cy4OOx_",
   "metadata": {
    "id": "G4CD3cy4OOx_"
   },
   "outputs": [],
   "source": [
    "# ! mkdir -p ~/.kaggle;\n",
    "# ! cp kaggle.json ~/.kaggle/kaggle.json\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04n3E3KnOOd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04n3E3KnOOd8",
    "outputId": "5c842f72-65bf-475b-85f7-8b75dea1f4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading h-and-m-personalized-fashion-recommendations.zip to /content\n",
      "100% 28.7G/28.7G [18:13<00:00, 29.2MB/s]\n",
      "100% 28.7G/28.7G [18:13<00:00, 28.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "#! kaggle competitions download -c h-and-m-personalized-fashion-recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w8n3-AtsOmMp",
   "metadata": {
    "id": "w8n3-AtsOmMp"
   },
   "outputs": [],
   "source": [
    "#!unzip /content/h-and-m-personalized-fashion-recommendations.zip -d \"drive/MyDrive/ds_project_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac94bbdf-2f9e-47e4-a077-e13801025cad",
   "metadata": {
    "id": "ac94bbdf-2f9e-47e4-a077-e13801025cad"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential   #, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Rescaling, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from tensorflow.keras.initializers import GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30f82df2-ead0-4316-a924-1d8d66b4e34a",
   "metadata": {
    "id": "30f82df2-ead0-4316-a924-1d8d66b4e34a"
   },
   "outputs": [],
   "source": [
    "def get_filepath(article_id):\n",
    "    \"\"\"\n",
    "    Returns the filepath of the image for the given article_id, e.g. 'samples/10/108775015.jpg'\n",
    "    article_id: (str) article_id e.g. '108775015'\n",
    "    \"\"\"\n",
    "    filepath = 'drive/MyDrive/ds_project_dataset/images/0' + article_id[:2] + '/' + '0' + article_id + '.jpg'\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5127870-a75b-455b-8bd3-175036579b4a",
   "metadata": {
    "id": "e5127870-a75b-455b-8bd3-175036579b4a"
   },
   "source": [
    "# Personalized Fashion Recommendations\n",
    "## Baseline model for image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c27f55-e6da-4629-90b4-435424b02fc8",
   "metadata": {
    "id": "42c27f55-e6da-4629-90b4-435424b02fc8"
   },
   "source": [
    "The images are RGB color coded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4825e-2345-415e-86bd-b11816b60ec2",
   "metadata": {
    "id": "50b4825e-2345-415e-86bd-b11816b60ec2"
   },
   "source": [
    "### Load the list of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "CZuBhHzQzJkF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZuBhHzQzJkF",
    "outputId": "17140557-65a7-4f52-dfed-08d7e44cf6c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105100"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows how many files in the images folder\n",
    "def list_files(dir):\n",
    "    \"\"\"\n",
    "    Returns list of filepaths in the given directory (dir) excluding hidden files\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if not file.startswith('.'):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "file_lst = list_files('drive/MyDrive/ds_project_dataset/images')\n",
    "len(file_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6d4d15c7-4f78-4baa-9670-13367bf5b285",
   "metadata": {
    "id": "6d4d15c7-4f78-4baa-9670-13367bf5b285"
   },
   "outputs": [],
   "source": [
    "with open(\"filename_product_group_name.txt\", \"r\") as file:\n",
    "    lines = file.readlines()[1:]\n",
    "    data = [line.strip().split(\",\") for line in lines]\n",
    "    df = pd.DataFrame(data, columns=[\"article_id\", \"product_group_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "nhug26mf4OFA",
   "metadata": {
    "id": "nhug26mf4OFA"
   },
   "outputs": [],
   "source": [
    "article_id_we_have = [path[-13:-4] for path in file_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "lqje8atP3bmO",
   "metadata": {
    "id": "lqje8atP3bmO"
   },
   "outputs": [],
   "source": [
    "# Filter out rows that we have not the correspondent file for them\n",
    "df = df[df['article_id'].isin(article_id_we_have)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "JDkEaC9r5aa3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDkEaC9r5aa3",
    "outputId": "0f93dfdf-3716-4028-fc9a-d73f884273c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104803"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9e292998-fe2b-4926-8494-12d38f61f1e8",
   "metadata": {
    "id": "9e292998-fe2b-4926-8494-12d38f61f1e8"
   },
   "outputs": [],
   "source": [
    "df['article_filepath'] = df['article_id'].apply(get_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fcc83-6266-46fa-8447-48411ea9ba9f",
   "metadata": {
    "id": "cd3fcc83-6266-46fa-8447-48411ea9ba9f"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "52ac3a08-20b4-45ce-9b81-453c20e87c99",
   "metadata": {
    "id": "52ac3a08-20b4-45ce-9b81-453c20e87c99"
   },
   "outputs": [],
   "source": [
    "# The product_group_name to have\n",
    "categories_to_have = \\\n",
    "['Garment Upper body', 'Garment Lower body', 'Garment Full body', 'Accessories', 'Underwear',\\\n",
    " 'Shoes', 'Swimwear', 'Socks & Tights', 'Nightwear']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(categories_to_have)\n",
    "labels = df['product_group_name']\n",
    "labels = pd.Series(data=le.transform(labels), index=labels.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c3189c66-a146-4b62-a9f5-5fa9521ca467",
   "metadata": {
    "id": "c3189c66-a146-4b62-a9f5-5fa9521ca467"
   },
   "outputs": [],
   "source": [
    "X = df['article_filepath']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d424b1-5332-4448-bb30-0ffd34b984ec",
   "metadata": {
    "id": "97d424b1-5332-4448-bb30-0ffd34b984ec"
   },
   "source": [
    "### Load and preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "nvTcJvv4r3mV",
   "metadata": {
    "id": "nvTcJvv4r3mV"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_SAMPLES_TO_USE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8909502f-37d8-48d0-9e57-3c6c9b97918c",
   "metadata": {
    "id": "8909502f-37d8-48d0-9e57-3c6c9b97918c"
   },
   "outputs": [],
   "source": [
    "# List of image paths\n",
    "#image_paths = [\"samples/sample_tshirt.jpg\", \"samples/sample_tshirt.jpg\"]\n",
    "image_paths = X[:NUMBER_OF_SAMPLES_TO_USE].values\n",
    "# List to store the loaded images\n",
    "images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4632788c-38c2-47fd-a8f7-b85bb9c81a1f",
   "metadata": {
    "id": "4632788c-38c2-47fd-a8f7-b85bb9c81a1f"
   },
   "outputs": [],
   "source": [
    "TARGET_SIZE = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "11MOglDu5tQs",
   "metadata": {
    "id": "11MOglDu5tQs"
   },
   "outputs": [],
   "source": [
    "# TODO: add try - exception if the image is not uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "efec3e56-f152-424a-a6d5-75b3ec508570",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efec3e56-f152-424a-a6d5-75b3ec508570",
    "outputId": "3cb77989-dc0a-4021-aa60-1bd8ef37e733"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:12<00:00, 23.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the images and convert them to numpy arrays\n",
    "for path in tqdm(image_paths):\n",
    "    image = load_img(path=path, target_size=TARGET_SIZE)\n",
    "    image = img_to_array(image)\n",
    "    images.append(image)\n",
    "\n",
    "# Convert the list of images to a numpy array\n",
    "images = np.stack(images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "q7djBud3sDbT",
   "metadata": {
    "id": "q7djBud3sDbT"
   },
   "outputs": [],
   "source": [
    "labels = labels[:NUMBER_OF_SAMPLES_TO_USE].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "M3IwEBFGuH5B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3IwEBFGuH5B",
    "outputId": "e6df199c-9cdf-4b64-88b7-3a54ea45afb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 100, 100, 3), (300,))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509814a-ca34-42b7-85ca-cdd144634c46",
   "metadata": {
    "id": "7509814a-ca34-42b7-85ca-cdd144634c46"
   },
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "63a8d43b-2446-43e5-a622-ce86081db2a7",
   "metadata": {
    "id": "63a8d43b-2446-43e5-a622-ce86081db2a7"
   },
   "outputs": [],
   "source": [
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad15805-a9df-412f-8686-ab5b1cfc98ac",
   "metadata": {
    "id": "7ad15805-a9df-412f-8686-ab5b1cfc98ac",
    "tags": []
   },
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8b8188b2-fc5d-4ebc-82d8-47b1aa80529a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b8188b2-fc5d-4ebc-82d8-47b1aa80529a",
    "outputId": "11f7da3b-16d1-4d31-a353-dc606824defb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_CLASSES = len(le.classes_)\n",
    "NUMBER_OF_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9c754160-1fdc-4ba3-bf3b-fd8b9034020c",
   "metadata": {
    "id": "9c754160-1fdc-4ba3-bf3b-fd8b9034020c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "baseline_model = Sequential(layers=[\n",
    "    # Rescale the image in the [0, 255] range to be in the [0, 1] range\n",
    "    Rescaling(scale=1./255, input_shape=images.shape[1:]),\n",
    "    \n",
    "    # The first convolutional layer\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    \n",
    "    # The max pooling layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the output\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(units=32, activation='relu'),\n",
    "    \n",
    "    # The output layer with units=NUMBER_OF_CLASSES. Sum of outputs equals to 1.\n",
    "    Dense(NUMBER_OF_CLASSES, activation='softmax')\n",
    "], name=\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "68999dab-bfd8-4fbd-a213-8da5d12f790d",
   "metadata": {
    "id": "68999dab-bfd8-4fbd-a213-8da5d12f790d"
   },
   "outputs": [],
   "source": [
    "# Check if the input and output of the model are correct\n",
    "assert baseline_model.input_shape[1:] == images.shape[1:]\n",
    "assert baseline_model.output_shape[1:] == (NUMBER_OF_CLASSES,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bf178c01-85f1-407f-abaa-4bda19a44bc8",
   "metadata": {
    "id": "bf178c01-85f1-407f-abaa-4bda19a44bc8"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9525b899-969c-4ebd-b54e-0315789fd567",
   "metadata": {
    "id": "9525b899-969c-4ebd-b54e-0315789fd567"
   },
   "outputs": [],
   "source": [
    "# Make Early Stopping and Checkpoints\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_model_baseline_CNN.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a874fa6-58eb-4317-9519-64abb67ab75c",
   "metadata": {
    "id": "4a874fa6-58eb-4317-9519-64abb67ab75c"
   },
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4f13a8f3-bcd6-4952-b96a-43d52d681fa7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4f13a8f3-bcd6-4952-b96a-43d52d681fa7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5d3d4eac-037b-4edf-ff07-3f078bb45d6c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.3290 - accuracy: 0.9259 - val_loss: 1.6719 - val_accuracy: 0.5556\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.2576 - accuracy: 0.9588 - val_loss: 2.0602 - val_accuracy: 0.5185\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.1898 - accuracy: 0.9671 - val_loss: 2.6164 - val_accuracy: 0.4444\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.1476 - accuracy: 0.9753 - val_loss: 2.8569 - val_accuracy: 0.3704\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.1409 - accuracy: 0.9794 - val_loss: 1.9977 - val_accuracy: 0.5556\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.1099 - accuracy: 0.9918 - val_loss: 2.4027 - val_accuracy: 0.4444\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 3s 193ms/step - loss: 0.1100 - accuracy: 0.9712 - val_loss: 2.3448 - val_accuracy: 0.5185\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.0863 - accuracy: 0.9794 - val_loss: 2.0980 - val_accuracy: 0.5926\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0734 - accuracy: 0.9918 - val_loss: 2.2492 - val_accuracy: 0.5926\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0616 - accuracy: 0.9959 - val_loss: 2.2811 - val_accuracy: 0.5556\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0784 - accuracy: 0.9877 - val_loss: 2.4622 - val_accuracy: 0.5556\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0599 - accuracy: 1.0000 - val_loss: 2.3251 - val_accuracy: 0.5185\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.0604 - accuracy: 0.9877 - val_loss: 2.2058 - val_accuracy: 0.5926\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 3s 195ms/step - loss: 0.0781 - accuracy: 0.9835 - val_loss: 2.3819 - val_accuracy: 0.4815\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.0703 - accuracy: 0.9918 - val_loss: 3.6341 - val_accuracy: 0.5185\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 2.7710 - val_accuracy: 0.5185\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 3.0897 - val_accuracy: 0.5185\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 2.8333 - val_accuracy: 0.5556\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 2.8877 - val_accuracy: 0.5185\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.9855 - val_accuracy: 0.5185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f614e2807c0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(x=images_train, y=labels_train, batch_size=16, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XX6GkX5au7zX",
   "metadata": {
    "id": "XX6GkX5au7zX"
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fd35d2bd-749a-471f-84d0-cc1c5aa09791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd35d2bd-749a-471f-84d0-cc1c5aa09791",
    "outputId": "f7fdcdb1-b60f-4ee5-a050-42222a951ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step - loss: 2.2206 - accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2205653190612793, 0.6000000238418579]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.evaluate(images_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JMBf1Nc_9ZTg",
   "metadata": {
    "id": "JMBf1Nc_9ZTg"
   },
   "source": [
    "The benchmark model trained on ~300 samples demonstrates accuracy 0.6 on the test dataset, which is better than the baseline model's accuracy 0.4 (share of the majority class Garment Upper Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8F6FOEuUy-nd",
   "metadata": {
    "id": "8F6FOEuUy-nd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ZDwsWMRy-iU",
   "metadata": {
    "id": "6ZDwsWMRy-iU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae01f8c-2c80-4714-8744-10699711ce32",
   "metadata": {
    "id": "7ae01f8c-2c80-4714-8744-10699711ce32",
    "outputId": "66916db8-a207-4426-b7de-318a44700baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kuzma 5b5f06b] the baseline model for image recognition update\n",
      " 1 file changed, 11 insertions(+), 10 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 578 bytes | 578.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/kuzmatsukanov/kuzma_omri_noa_data_project.git\n",
      "   552bd95..5b5f06b  kuzma -> kuzma\n"
     ]
    }
   ],
   "source": [
    "# !git branch\n",
    "# !git add .\n",
    "# !git commit -m \"the baseline model for image recognition update\"\n",
    "# !git push origin kuzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266e13f-3d7c-44c4-8c3e-d4c1244585dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
